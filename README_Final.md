# Lending Club Loan Approval Policy Optimization
## Policy Learning: Supervised Deep Learning vs. Offline Reinforcement Learning

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0-red)
![Status](https://img.shields.io/badge/Status-Complete-brightgreen)
![License](https://img.shields.io/badge/License-MIT-green)

A comprehensive end-to-end machine learning project comparing **Supervised Deep Learning** for risk prediction and **Offline Reinforcement Learning** for return maximization in loan approval optimization using the Lending Club dataset (1.05M+ loans).

---

## üéØ Project Overview

This project implements two complementary machine learning paradigms to optimize loan approval decisions:

| Aspect | Supervised DL | Offline RL |
|--------|---------------|-----------|
| **Objective** | Predict default risk | Maximize financial return |
| **Key Metrics** | AUC=0.7105, F1=0.4466 | EPV (Estimated Policy Value) |
| **Policy Type** | Threshold-based probability | Value-based Q-learning |
| **Status** | ‚úÖ Production-ready | ‚ö†Ô∏è Requires fixes |
| **Deployment** | Deploy immediately | Fix & re-test (Phase 2) |

### üìä Key Results

**Supervised Deep Learning Model:**
- **AUC-ROC**: 0.7105 (71% probability of correct risk ranking)
- **F1-Score**: 0.4466 (66% recall, 34% precision)
- **Default Rate (Approved)**: ~12% (vs. 21.5% baseline)
- **Approval Rate**: ~50% (at threshold œÑ=0.35)
- **Architecture**: 4-layer MLP, 61,377 parameters

**Offline RL Agent:**
- **Training Status**: Training challenges (Q-values converged to zero)
- **Root Cause**: Unnormalized rewards ‚Üí gradient explosion
- **Lesson**: Offline RL requires Conservative Q-Learning, target networks
- **Fix Plan**: Implement CQL in Phase 2

---

## üìÅ Repository Structure

```
lending-club-policy-optimization/
‚îÇ
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ requirements.txt                   # Python dependencies
‚îú‚îÄ‚îÄ All_codes.ipynb                    # Complete notebook (Tasks 1-4)
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                         # Individual task notebooks
‚îÇ   ‚îú‚îÄ‚îÄ 01_EDA_and_Preprocessing.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_Supervised_DL_Model.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_Offline_RL_Agent.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 04_Analysis_and_Comparison.ipynb
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ README_DATA.md                 # Dataset download instructions
‚îÇ   ‚îî‚îÄ‚îÄ raw/                           # (Not in repo - download separately)
‚îÇ
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ Task4_Final_Report.md
‚îÇ   ‚îî‚îÄ‚îÄ Task4_Final_Report.pdf
‚îÇ
‚îî‚îÄ‚îÄ results/
    ‚îú‚îÄ‚îÄ metrics.csv
    ‚îî‚îÄ‚îÄ plots/
```

---

## üöÄ Quick Start (5 Minutes)

### 1Ô∏è‚É£ Clone Repository
```bash
git clone https://github.com/yourusername/lending-club-policy-optimization.git
cd lending-club-policy-optimization
```

### 2Ô∏è‚É£ Create Virtual Environment
```bash
# Using venv (Mac/Linux)
python -m venv venv
source venv/bin/activate

# Or using conda (Windows/Mac/Linux)
conda create -n lending-club python=3.9
conda activate lending-club
```

### 3Ô∏è‚É£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4Ô∏è‚É£ Download Dataset
```bash
# Download from Kaggle: https://www.kaggle.com/datasets/wordsforthewise/lending-club
# Place accepted_2007_to_2018Q4.csv.gz in: data/raw/

mkdir -p data/raw
# Move your downloaded file here
```

### 5Ô∏è‚É£ Run Jupyter Notebook
```bash
jupyter notebook All_codes.ipynb
# Or run individual task notebooks:
jupyter notebook notebooks/01_EDA_and_Preprocessing.ipynb
```

---

## üìã Detailed Setup Instructions

### Prerequisites
- **Python**: 3.8 or higher
- **Git**: For cloning repository
- **Kaggle Account**: To download dataset (free)
- **Disk Space**: ~5GB (for data + models)
- **RAM**: 8GB minimum, 16GB recommended
- **GPU (Optional)**: CUDA 11.8+ for faster training

### Step-by-Step Installation

#### Option A: Using Virtual Environment (venv)
```bash
# 1. Create virtual environment
python3 -m venv venv

# 2. Activate virtual environment
source venv/bin/activate              # Mac/Linux
# OR
venv\Scripts\activate                 # Windows

# 3. Upgrade pip
pip install --upgrade pip

# 4. Install dependencies
pip install -r requirements.txt

# 5. Verify installation
python -c "import torch; print(f'PyTorch version: {torch.__version__}')"
python -c "import pandas; print(f'Pandas version: {pandas.__version__}')"
```

#### Option B: Using Conda
```bash
# 1. Create conda environment
conda create -n lending-club python=3.9 -y

# 2. Activate environment
conda activate lending-club

# 3. Install dependencies
pip install -r requirements.txt

# 4. Verify
python -c "import torch; print(torch.__version__)"
```

#### Option C: Direct pip (Not Recommended)
```bash
pip install -r requirements.txt
```

### Troubleshooting

**Issue**: `ModuleNotFoundError: No module named 'torch'`
```bash
pip uninstall torch -y
pip install torch torchvision
```

**Issue**: CUDA out of memory during training
```
# In notebook: reduce batch_size from 512 to 256 or 128
# Edit cell: batch_size = 256
```

**Issue**: Dataset not found
```bash
# Make sure file is in correct location
ls -la data/raw/accepted_2007_to_2018Q4.csv.gz
# Should show the file, if not download from Kaggle
```

---

## üìä Notebook Contents

### `All_codes.ipynb` (Complete Project)
Complete end-to-end implementation in a single notebook:

**Task 1: Exploratory Data Analysis (20 min)**
- Load 2,260,701 loans from Lending Club
- Analyze 151 features and missing values
- Feature engineering and encoding
- Binary target creation (21.5% default rate)
- Train-test split: 80-20 with stratification
- **Output**: Cleaned dataset (1,373,508 √ó 66 features)

**Task 2: Supervised Deep Learning (30 min)**
- 4-layer MLP architecture: 150‚Üí256‚Üí128‚Üí64‚Üí32‚Üí1
- BatchNormalization + Dropout for regularization
- Weighted BCEWithLogitsLoss for class imbalance
- Training with ReduceLROnPlateau scheduler
- **Results**: AUC=0.7105, F1=0.4466, Recall=66%

**Task 3: Offline RL Agent (25 min)**
- Q-Network: 150‚Üí256‚Üí128‚Üí64‚Üí2 (Q-values for actions)
- MSELoss for Q-learning objective
- Reward structure: Deny=$0, Approve+Paid=profit, Approve+Default=-loss
- Training challenges documented
- **Status**: Demonstrates offline RL difficulties

**Task 4: Analysis & Comparison (15 min)**
- Policy comparison (DL threshold vs. RL value-based)
- Decision examples (high-risk, moderate-risk, low-risk applicants)
- Why policies diverge
- Deployment roadmap (4 phases)
- Limitations and future work

### Individual Notebooks (Optional)
If you prefer separate files:
- `01_EDA_and_Preprocessing.ipynb` - Data analysis only
- `02_Supervised_DL_Model.ipynb` - DL model training
- `03_Offline_RL_Agent.ipynb` - RL implementation
- `04_Analysis_and_Comparison.ipynb` - Results synthesis

---

## üèóÔ∏è Model Architecture Details

### Supervised Deep Learning Classifier

**Architecture:**
```
Input Layer (150 features after encoding)
    ‚Üì
BatchNorm + Linear(150‚Üí256) + ReLU + Dropout(0.3)
    ‚Üì
BatchNorm + Linear(256‚Üí128) + ReLU + Dropout(0.3)
    ‚Üì
BatchNorm + Linear(128‚Üí64) + ReLU + Dropout(0.3)
    ‚Üì
Linear(64‚Üí32) + ReLU + Dropout(0.3)
    ‚Üì
Linear(32‚Üí1) + Sigmoid Output
```

**Training Configuration:**
```python
Loss:           BCEWithLogitsLoss (weighted for class imbalance)
Optimizer:      Adam (learning_rate=0.001)
Batch Size:     512
Epochs:         50
Scheduler:      ReduceLROnPlateau (factor=0.5, patience=3)
Regularization: L2 weight decay (1e-5), Dropout (0.3)
```

**Performance:**
```
Metric          Value   Interpretation
AUC-ROC         0.7105  71% correct risk ranking
F1-Score        0.4466  66% recall, 34% precision
Recall          66%     Catches 2 of 3 defaults
Precision       34%     Only 34% of flagged loans default
False Positive   35%    Incorrectly flags 35% of good loans
Default Rate     12%    Reduced from 21.5% baseline
```

### Offline RL Q-Network

**Architecture:**
```
Input State (150 features)
    ‚Üì
BatchNorm + Linear(150‚Üí256) + ReLU + Dropout(0.2)
    ‚Üì
Linear(256‚Üí128) + ReLU + Dropout(0.2)
    ‚Üì
Linear(128‚Üí64) + ReLU
    ‚Üì
Linear(64‚Üí2) Output (Q-values for [Deny, Approve])
```

**Reward Function:**
```python
if action == Deny:
    reward = $0  # No risk, no gain

elif action == Approve:
    if loan_paid_off:
        reward = loan_amount √ó interest_rate  # Profit
    else:
        reward = -loan_amount  # Loss of principal
```

---

## üìà Results and Evaluation

### Performance Comparison

| Metric | DL Model | RL Agent | Winner |
|--------|----------|---------|--------|
| AUC-ROC | 0.7105 | N/A | DL |
| F1-Score | 0.4466 | N/A | DL |
| Approval Rate | ~50% | ~0% | N/A |
| Training Status | ‚úÖ Converged | ‚ùå Failed | DL |
| Deployment Ready | ‚úÖ Yes | ‚ùå No | DL |

### Generated Outputs
- `results/metrics.csv` - DL model performance metrics
- `results/plots/roc_curve.png` - ROC-AUC visualization
- `results/plots/training_history.png` - Loss over epochs
- `results/plots/confusion_matrix.png` - Prediction breakdown
- `results/plots/policy_comparison.png` - DL vs. RL comparison

---

## üîÑ Data Pipeline

### 1. Load and Explore
```python
# Load dataset
df = pd.read_csv('data/raw/accepted_2007_to_2018Q4.csv.gz', 
                  compression='gzip')

# Dataset shape: (2,260,701, 151)
# Target distribution: 78.5% Fully Paid, 21.5% Defaulted
```

### 2. Clean and Preprocess
```python
# Drop columns with >99% missing
# Convert loan_status to binary: 0=Paid, 1=Default
# Encode categorical variables (one-hot encoding)
# Scale numeric features (StandardScaler)

# Final shape: (1,373,508, 66)
```

### 3. Split and Prepare
```python
# Train-test split: 80-20
# Stratified split to maintain class distribution
# Convert to PyTorch tensors

# Train: 1,098,806 samples
# Test: 274,702 samples
```

### 4. Train and Evaluate
```python
# DL Model: AUC=0.7105, F1=0.4466
# RL Agent: Training challenges documented
# Compare policies and recommendations
```

---

## üöÄ Deployment Roadmap

### Phase 1: Immediate (Deploy DL Model)
- **Timeline**: Now
- **Policy**: Approve if P(default) < 0.35
- **Expected**: 50% approval, 12% default rate, 3x profit improvement
- **Monitoring**: Monthly approval rates, default rates

### Phase 2: Medium-Term (Fix RL, 3-4 months)
- **Action**: Implement Conservative Q-Learning (CQL)
- **Fixes**: Reward normalization, target networks, L2 regularization
- **Goal**: Trained RL policy with positive EPV

### Phase 3: A/B Testing (6 months)
- **Split**: 70% DL (œÑ=0.35), 30% improved RL
- **Metrics**: Approval rate, default rate, profit
- **Decision**: Which policy performs better?

### Phase 4: Production (12+ months)
- **Action**: Deploy winner to 100% of applicants
- **Monitoring**: Monthly fairness audits, model drift detection
- **Iteration**: Quarterly retraining

---

## üî¨ Reproducibility

### Exact Reproduction

To reproduce results identically:

1. **Use exact environment:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Random seeds (set in notebook):**
   ```python
   import numpy as np
   import torch
   np.random.seed(42)
   torch.manual_seed(42)
   ```

3. **Run notebooks sequentially:**
   ```bash
   # All in one:
   jupyter notebook All_codes.ipynb
   
   # Or individually:
   jupyter nbconvert --to notebook --execute notebooks/01_EDA_and_Preprocessing.ipynb
   jupyter nbconvert --to notebook --execute notebooks/02_Supervised_DL_Model.ipynb
   jupyter nbconvert --to notebook --execute notebooks/03_Offline_RL_Agent.ipynb
   jupyter nbconvert --to notebook --execute notebooks/04_Analysis_and_Comparison.ipynb
   ```

4. **Verify outputs:**
   ```bash
   cat results/metrics.csv
   # Should show: AUC=0.7105, F1=0.4466
   ```

---

## üìö Key Findings

### Why AUC=0.7105 is Good
- **Industry Benchmark**: AUC > 0.70 is good; > 0.75 is excellent
- **Class Imbalance**: With 21.5% default rate, AUC is more meaningful than accuracy
- **Risk Ranking**: Shows model genuinely learns to rank risky loans higher

### Why F1=0.4466 Reveals Trade-offs
- **66% Recall**: Catches 2 of 3 defaults (reduces losses)
- **34% Precision**: Only 34% of flagged loans actually default (false alarms)
- **Business Impact**: Rejects 100 loans to avoid 34 defaults, wrongly rejects 66 good loans

### Why Policies Diverge
- **DL Policy**: Risk-focused, threshold-based (approve if P(default) < 0.35)
- **RL Policy**: Return-focused, value-based (approve if Q(Approve) > Q(Deny))
- **Example**: High-income applicant with 55% default risk
  - DL: DENY (too risky)
  - RL: APPROVE (income offsets risk, positive expected value)

### Lessons from RL Training Failure
- Offline RL is harder than supervised learning
- Requires Conservative Q-Learning, target networks, proper normalization
- Valuable insights even when training fails

---

## üîÆ Future Work

### Algorithm Improvements
- [ ] Implement Conservative Q-Learning (CQL) for offline RL
- [ ] Try XGBoost/LightGBM on tabular data
- [ ] Explore Batch-Constrained Q-Learning (BCQ)
- [ ] Test AWAC (Advantage-Weighted Actor-Critic)

### Data Enhancements
- [ ] Employment history and job stability
- [ ] Payment timeliness and consistency
- [ ] Alternative credit signals (rent, utilities)
- [ ] Macroeconomic factors (unemployment, housing prices)

### Fairness & Compliance
- [ ] Implement SHAP/LIME for explanations
- [ ] Monitor approval rates by demographic groups
- [ ] Add fairness constraints (demographic parity)
- [ ] Quarterly disparate impact audits

---

## üìñ Documentation

- **README.md** (this file) - Project overview and setup
- **data/README_DATA.md** - Dataset download and structure
- **reports/Task4_Final_Report.md** - Comprehensive 3-page analysis
- **notebooks/** - Individual task notebooks with detailed comments

---

## ü§ù Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -am 'Add improvement'`)
4. Push to branch (`git push origin feature/improvement`)
5. Submit pull request

---

## üìÑ License

MIT License - see LICENSE file for details

---

## üìß Questions?

- **Issues**: Use GitHub Issues for bug reports
- **Discussions**: Use GitHub Discussions for questions
- **Email**: [your-email@example.com]

---

## üôè Acknowledgments

- **Dataset**: Lending Club (https://www.lendingclub.com/)
- **Libraries**: PyTorch, scikit-learn, pandas, numpy
- **References**: 
  - Fawcett, T. (2006). "An introduction to ROC analysis"
  - Kumar, A., et al. (2020). "Conservative Q-Learning for Offline RL"
  - Goodfellow, I., et al. (2016). "Deep Learning"

---

## üìä Citation

```bibtex
@misc{lending_club_policy_2025,
  title={Policy Optimization for Financial Decision-Making: 
         Supervised Learning vs. Offline Reinforcement Learning},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/lending-club-policy-optimization}
}
```

---

## ‚úÖ Getting Started Checklist

- [ ] Clone repository
- [ ] Create virtual environment
- [ ] Install dependencies (`pip install -r requirements.txt`)
- [ ] Download dataset from Kaggle
- [ ] Place dataset in `data/raw/`
- [ ] Open `All_codes.ipynb` in Jupyter
- [ ] Run all cells sequentially
- [ ] Review results and analysis
- [ ] Check final report in `reports/`

---

**Ready to explore? Start with `All_codes.ipynb` or run individual task notebooks!** üöÄ

**Last Updated**: October 30, 2025  
**Status**: ‚úÖ Complete and Ready for Review